## AI洞察日报 2026/2/28

>  `AI 日报` 

### 今日摘要

【1】🤨 最小 transformer 加两 10 位数：36、311、28 参数与可复现性争议
原标题： 《Smallest transformer that can add two 10-digit numbers》 评分: 34 | 作者: ks2048 💭 36 个参数就能学十位数相加？真的假的？ 🎯 讨论背景 原帖讨论一个号称能用极小 transformer 完成对两个 10 位数相加的实现，社区关注点集中在极小参数量的可行性与可复现性。评论中提到不同参数数字（如 28、36、311）和据称的 >=99% 准确率，引发对手写权重、训练流程和实验透明度的质疑。讨论涉及技术细节：是否把该功能当作可替换的固定模块嵌入到 LLM（large language model）预训练流程、tokenize（分词）如何影响输入表示，以及单次 matmul（矩阵乘法）示例为何只是戏谑式简化。总体而言，争论横跨实现与定义层面：到底是演示可训练的最小模型，还是把算法包装成"模型”以博眼球。 📌 讨论焦点 模块化与权重可替换性争论 有评论提出是否可以在 LLM（large language model）预训练前把这种单用途且权重固定的网络嵌入，以复用相同的推理代码并节省资源。另一条评论引用了一个判定标准：如果能替换权重并使用相同的推理代码则算"合法”的模型，反之若推理代码与算法不可分离则不算模型，这将"模型”与"算法实现”区分开来。讨论还关注手写/固定权重（例如 36 个参数）与训练后权重（例如 311 个参数）之间的差异，并质疑是否有人从随机初始化开始训练该结构以验证其可学习性。总体关切在于模块化、固定权重与"可替换权重”范式如何影响结果的解释与可复现性。 [来源1] [来源2] [来源3] 参数规模与可复现性怀疑 多位评论对参数计数和报告精度提出质疑：有人在 Twitter 上看到称只需 28 个参数的说法，而讨论中也出现了手写权重 36 个、训练后 311 个等不同数字。评论对">=99% accuracy”感到惊讶并怀疑其可信度，尤其在演示被描述为"vibe coded”（凭直觉/手工）且没有提交到 arXiv（预印本服务器）的情况下。评论者要求公开实现细节、训练设置与从随机初始化训练的实验结果，以便验证这些极小模型的可训练性与鲁棒性。社区强调僅有参数或准确率数字不足以说服人，必须有完整训练流程和可复现代码。 [来源1] [来源2] [来源3] 把简单矩阵乘法等同于 Transformer 的误解与笑话 有人以戏谑方式指出两个数相加在数学上可以用一次 matmul（矩阵乘法）实现，并举出 [A B] × [1;1] = [A +B] 的示例来讽刺过度简化的说法。随后有评论指出这是个玩笑，因为 transformer 在处理输入前会先 tokenize（分词/切片），随后在 token 表示上执行一系列 matmul、激活（如 ReLU）和 attention 操作，模型并不会直接"看到”原始数字字符串。还有人把把任意 transformer 转换成紧凑低功耗门电路的设想质疑为对内部机制的过度抽象或误解。讨论提醒读者区分数学上单步线性代数运算与深度模型中输入编码和层级处理的差别。 [来源1] [来源2] [来源3] [来源4] [来源5] [来源6] 对演示包装与噱头的嘲讽 评论中有人讽刺性地建议把成果打包成 Electron（桌面应用框架）应用，以调侃将小实验商品化或展示化的趋势。另一条评论表达了对缺乏严谨评估（例如无论文支持、凭直觉实现）的疲惫，表示要把注意力留给更可靠的研究。这些反应反映出社区对噱头式展示和不透明报告的不满，倾向于优先要求可验证、可重复的技术说明而非表面参数游戏或花哨包装。 [来源1] [来源2] 📚 术语解释 transformer: transformer（Transformer）是一种基于 self-attention 的序列建模神经网络架构，依赖 token 表示、多层矩阵运算和注意力机制来处理文本或序列数据。 matmul: matmul 是 'matrix multiplication' 的简称，指矩阵乘法，深度学习中用于实现线性变换和前向推理的基本线性代数运算。 tokenize: tokenize（分词/切片）是将原始输入（如数字字符串或文本）拆分为模型可处理的 tokens 的过程，直接影响模型能否以合适的表示"看见”输入。 hand-coded weights（手写/固定权重）: hand-coded weights 指由人或规则直接设定的网络权重，而非通过梯度训练得到的参数；常用于演示可否用固定参数实现某种计算。 params: params（parameters）指模型中的可学习参数数量，常用于衡量模型规模，但参数数量本身不能完全反映模型的可训练性或实际性能。

【2】🛡️ Anthropic 回应 Hegseth：称供应链风险指定仅限军事合同并拒绝被全面封杀
原标题： 《Not Found》 评分: 22 | 作者: surprisetalk 💭 是要把 Claude 当作核武机密管制吗？ 🎯 讨论背景 本讨论发端于 Anthropic 就公众人物 Pete Hegseth 对公司和其模型的评论所做的公开回应。声明援引了 10 USC 3252（美国法典第 10 编第 3252 节），并主张 supply chain risk designation 在法律上只可影响 Department of War 合同中对 Claude（Anthropic 的大型语言模型）的使用，而不应自动切断商业客户或个人用户的访问。评论者围绕法律条文、行政执行力与现实施压手段展开，提到可能被施压的环节包括国防承包商与为模型提供算力的云厂商（如 Google），以及政府以 restricted data 名义扩大管控的可能性。讨论同时涉及工程师的职业伦理、国家安全话语作为谈判工具的运用，以及司法先例能否实际遏制行政权力扩张。 📌 讨论焦点 支持与道德立场 多位评论者赞赏 Anthropic 公开承认无法准确预测强大模型被滥用的方式，认为这种透明与谨慎是一种负责任的公司姿态。有人把公司的声明形容为礼貌但坚决的反抗，认为在原则问题上不妥协会赢得公众信任并在长期竞争中占优。评论还把问题提升到个人职业伦理层面，建议若工作会直接助长监控或伤害无辜，应当认真考虑是否继续从事。简短的支持言论也表明这类立场能为公司赢得用户和舆论好感。 [来源1] [来源2] [来源3] [来源4] 法律边界与执行风险 Anthropic 的声明援引了 10 USC 3252，指出 supply chain risk designation 在法律上只能针对 Department of War 合同中对 Claude 的使用进行限制，不能自动影响公司对商业客户或个人用户的服务。评论者认为尽管法律文字有限，行政与政治手段仍能在实践中扩大影响力：政府可能通过强迫承包商切断合作、对提供算力的云供应商施压（例如 Google），甚至以将技术列为 restricted data 的方式实施更严厉的限制。有人提到历史上有律所因行政命令而诉诸法庭并胜诉，但也有评论对司法或制度能否制约行政滥权抱持怀疑，担心实际"爆破半径”会比法律条文更大。总体观点既承认法律适用范围的狭窄，也对行政执行力和政治压力带来的实务风险表示忧虑。 [来源1] [来源2] [来源3] [来源4] 国家安全话语与谈判策略的怀疑 部分评论质疑官方以"危害美国战士与平民”为由的表述，认为这种国家安全论调常被用作谈判策略而非单纯的技术评估。有人指出公开信件中使用"爱国”或"国家安全”论述，往往是为了在政治与舆论层面施压，对外界而言应把这些陈述看作谈判的一部分。评论还提醒，涉及 autonomous weapons（自主武器）等议题时，国家安全关切可能被选择性地引用，而对他国平民风险的忽视暴露了论述的双重标准。整体上，评论者建议区分技术风险评估与政治话语的工具性使用。 [来源1] [来源2] 监控、后门与工程师道德 有评论提出对政府是否已拥有对科技公司"默认后门”的担忧，提到 NSA（美国国家安全局）或其他机构可能历史上或法律上有要求访问的先例，从而使所谓的管制具备现实可执行性。基于这种可能性，一些人主张工程师和技术从业者在选择项目时应考虑是否在帮助"big brother”扩大可见性或能力，必要时应拒绝参与可能导致监控或武器化的工作。这些观点把技术合规问题与个人伦理责任联系起来，认为法律与政策不足以完全替代职业与道德判断。 [来源1] [来源2] 媒体标题、先例与升级风险 部分评论指出原帖或报道标题不准确，强调这次声明更像是向客户澄清针对 Hegseth 社交帖子的回应，而非政府政策的最终定论。讨论中也提到并非首例与行政当局对峙的情形——此前有律所针对行政命令提起诉讼并胜诉——但同时有人担心公开点名与指责会引发进一步升级，扩大政治博弈的范围。总体观点认为法律先例能提供一定保护，但公开对抗本身仍会带来额外风险与不确定性。 [来源1] [来源2] [来源3] 📚 术语解释 Claude: Anthropic 的大型语言模型（LLM），讨论中公司针对该模型的使用与法律影响为核心争议点。 10 USC 3252: 美国法典第 10 编第 3252 节，涉及国防部就供应链风险对供应商或技术作出指定的法律条款，法律适用主要限于国防合同范围。 supply chain risk designation: 供应链风险指定：一种行政或法律认定，用以在国防合同中对被视为风险的供应商或技术施加限制或附加条件。 restricted data: 限制数据（restricted data）：用于核技术等高度敏感信息的分类标签；若将软件或模型归入该类别，会触发严格访问控制和法律后果。 Executive Orders (EOs): 行政命令（EOs）：总统或行政机构发布的指令，可直接影响联邦政策与合同执行，历史上常成为司法审查对象。 government backdoor / 后门: 政府后门：指政府要求企业提供绕过加密或直接访问系统与数据的机制，此类要求在隐私、安全与法律上长期存在争议。 类别： AI | Policy | Business | Opinion | Anthropic | Claude | Pete Hegseth | supply chain risk designation | Department of War | claude.ai

【3】😬 "We Will Not Be Divided”：Anthropic 员工团结誓言与伦理能否在现实中存活？
原标题： 《We Will Not Be Divided》 评分: 22 | 作者: BloondAndDoom 💭 真要等良心吃不起饭、公司才放弃利润吗？ 🎯 讨论背景 讨论源自一份题为 "We Will Not Be Divided” 的公开宣言——来自 AI 公司内部人员的团结与伦理呼吁，评论里多次提到 Anthropic（AI 研究公司）和 xAI（AI 初创公司）。参与者以历史案例（例如 Google 在监控/监视项目上的让步）与对 DoD（美国国防部）或其他国家机构可能绕过企业伦理承诺的担忧为前提展开争论。讨论交织了对企业利润驱动本质的现实主义分析、对传播措辞与法律框架的策略性关注，以及能否把承诺扩大到更广泛人群或跨国推广的可行性评估。总体语境是技术从业者在 AI 军事化、监管压力与企业生存之间寻求平衡的焦虑与希望。 📌 讨论焦点 支持与团结：员工誓言与扩展签名的呼吁 若干评论对原文表示赞赏，称其勇敢、温暖并呼吁在当前形势下坚持这道德底线。有人明确建议扩大行动范围，提出在 xAI 等公司收集签名以填补可能出现的空位，认为行业内应当有一处能够坚持伦理的阵地。评论中的口号式呼声（如"Hold this line”）反映出希望建立并守住一个示范性的道德立场的愿望。 [来源1] [来源2] [来源3] 现实主义与怀疑：公司能否兼顾道德与生存？ 不少评论怀疑企业在美国能否同时保持道德并生存，直言如果 Anthropic 倒下就会成为"不能兼顾道德与成功”的证明。有人援引历史先例指出大公司往往会妥协，例如十年前 Google 在监控/监视相关项目上的让步，暗示伦理承诺易受商业或国家压力侵蚀。更激烈的观点认为公司本质受利润驱动（"money is the only true God” 的论调），并警告公众舆论或选民可能会通过监管改变行业走向，同时提醒要为"希望未能实现”的情形做准备。 [来源1] [来源2] [来源3] [来源4] 扩展承诺的可行性与国际挑战 部分评论讨论是否应把此类誓言扩展到所有美国人甚至全球，提出不应局限于某几家公司的员工。反对者指出全球推广难度极高，尤其是在中国、印度、俄罗斯或中东等政治环境迥异的地区，需要巨大的勇气与不同体制下的承受力。与此同时也有人认为开发者群体具有"cybernetic”特质，行业内部仍有可能在跨国或跨公司层面形成某种共识或压力。 [来源1] [来源2] 措辞与政治框架问题 有人针对文本措辞提出策略性批评，指出使用诸如 "Department of War” 等表述会无意中采用对方话语并在法律/政治层面预先让步。评论提醒在与军事化或政府政策对立时应注意术语的准确性，以免在论证或公众传播中丧失立场正当性。这种关注不是空泛的语义争辩，而是对传播效果和法律框架敏感性的实际策略性建议。 [来源1] 对军方绕行的担忧与讽刺调侃 部分评论以讽刺或调侃表达对军方或政府会绕过企业伦理承诺的悲观，例如戏言"DoD ^HW will just use DeepSeek”，暗指美国国防部会自研或采购替代工具。与此同时也有轻松或怀旧的梗（如"He will not divide us!”、对 Club Penguin 与 Roblox 的比较），这些笑话既是情绪发泄也反映社区的无奈与疲惫。总体上，这类评论把注意力从理想化的企业自律拉回到国家机器与现实政治能否被企业承诺左右的问题上。 [来源1] [来源2] [来源3] [来源4] [来源5] 类别： AI | Work | Product | Opinion | AI | notdivided.org | Google

【4】A statement on the comments from Secretary of War Pete Hegseth. https://anthropic.com/news/statement-comments-secretary-war
A statement on the comments from Secretary of War Pete Hegseth. https://anthropic.com/news/statement-comments-secretary-war

【5】打破英伟达垄断！Meta签署数两百亿美元大单，改租谷歌TPU自研AI模型
在AI芯片领域，一场旨在"去英伟达化”的巨头博弈正在上演。社交媒体巨头Meta近日与谷歌达成了一项跨年度、价值达数十亿美元的重磅协议，计划租用谷歌自研的张量处理单元（TPU）来开发其新一代AI模型。 这一动作直接挑战了英伟达在AI芯片市场的统治地位。长期以来，英伟达一直是Meta训练模型时的 首选 供应商，Meta甚至在几天前刚宣布要从英伟达和AMD购买数百万个GPU。然而，Meta此次"脚踏两只船”租用谷歌TPU，不仅是为了缓解算力焦虑，更是为了在自研数据中心中探索除GPU之外的替代方案，据悉Meta甚至考虑从明年开始直接购买TPU。 谷歌的"算盘”：既是客户，也是对手 这场交易背后的逻辑颇为微妙。谷歌云高管已设定目标，计划通过销售TPU夺取英伟达约10%的年收入（约 200 亿美元）。为了实现这一目标，谷歌不仅与投资机构合作对外租赁TPU，更试图通过差异化竞争吸引像OpenAI、Meta这样的大客户。 有趣的是，由于云端用户对GPU的需求依然强劲，谷歌本身仍是英伟达 最大 的客户之一。它必须一边斥巨资购买英伟达的 最新 芯片以保持云市场竞争力，一边推销自家的TPU来蚕食英伟达的市场份额。 市场连锁反应：倒逼芯片降价 AI芯片市场的这种"内卷”对下游开发者而言显然是件好事。据行业消息称，正是因为TPU等替代品的存在，OpenAI在与英伟达的谈判中成功压低了30%的采购价格。 随着Meta等巨头开始大规模转向多元化算力布局，英伟达一家独大的局面正面临前所未有的压力。这场关于算力底座的"军备竞赛”，正从单纯的产能比拼演变为架构与生态的全面较量。

【6】​AI音乐也疯狂！Suno付费订阅突破 200 万，年收入冲向 3 亿美元大关
AI音乐生成领域正迎来爆发式增长。近日，知名AI音乐创作平台Suno的联合创始人兼首席执行官Mikey Shulman披露了公司的 最新 经营数据：Suno目前的付费订阅用户数已正式突破 200 万 ，年经常性收入（ARR）更是达到了惊人的 3 亿美元 。 回顾三个月前，Suno在完成2. 5 亿美元融资时的估值为24. 5 亿美元，当时披露的年收入仅为 2 亿美元。这意味着在短短一个季度内，Suno的营收规模就实现了**50%**的跨越式增长。 Suno之所以能快速出圈，核心在于其极低的创作门槛。用户只需输入简单的自然语言提示词，AI就能在几秒钟内生成旋律优美、人声逼真的完整歌曲。 这种"让每个人都能写歌”的能力已经开始重塑音乐产业。例如， 31 岁的密西西比州用户Telisha Jones通过Suno将自己的诗歌转变成了R&#x26;B金曲《How Was I Supposed to Know》，不仅在社交媒体上走红，更获得了一份价值 300 万美元 的唱片合约。 尽管发展迅猛，Suno也面临着来自传统音乐行业的挑战。包括Billie Eilish、Katy Perry在内的多位 顶级 音乐人曾公开抵制AI对音乐作品的侵权。此前，多家唱片公司也因版权问题对Suno提起诉讼。 不过，局面正在发生积极变化。华纳音乐集团近期已与Suno达成和解，并签署了授权协议，允许Suno使用其曲库中的音乐来训练新的AI模型。这意味着AI音乐平台正在从行业的"搅局者”转变为"合作伙伴”。

【7】Claude Code 下个版本将新增两项 Skills: /simplify 和 /batch 1. /simplify 作用：在代码变更之后，自动调用多个并行 Agent，对代码进行质量提升、性能调优，并...
Claude Code 下个版本将新增两项 Skills: /simplify 和 /batch 1. /simplify 作用：在代码变更之后，自动调用多个并行 Agent，对代码进行质量提升、性能调优，并确保符合 CLAUDE. md 的规范。 使用方式：在完成一次代码修改后，直接对 Claude 说 "hey claude make this code change then run /simplify”。 核心作用：把原本需要人工反复 review、优化、合规检查的工作，变成一键并行自动化处理，大幅减少 PR 合并前的"打磨”时间。 2. /batch 作用：支持交互式规划 + 并行批量执行的大规模代码迁移。 · 先由用户与主 Agent 交互式制定迁移计划； · 随后自动生成数十个独立 Agent，每个 Agent 负责迁移的一部分； · 每个 Agent 都在完全隔离的 git worktree 中运行； · 完成本地测试后，才自动创建 PR。 使用示例： · "/batch migrate src/ from Solid to React” · "/batch migrate from jest to vite” 核心作用：让过去需要数天甚至数周的手动重构（框架升级、库替换、目录迁移等），变成几分钟到几小时的自动化并行作业，且安全性更高。 [图片: https://pbs.twimg.com/media/HCNfbWHbEAAJ5Sk?format=jpg&#x26;name=orig] Boris Cherny: In the next version of Claude Code.. We're introducing two new Skills: /simplify and /batch. I have been using both daily, and am excited to share them with everyone. Combined, these kills automate much of the work it used to take to (1) shepherd a pull request to production [图片: https://pbs.twimg.com/media/HCMgWZ2bUAA2hq9?format=png&#x26;name=orig]

【8】学会像 Agent 一样看：Claude Code 工具设计实践 来自 Claude Code 开发者 @trq212，记录了 Anthropic 团队在开发 Claude Code 过程中关于 Agent 工具设计 的实...
学会像 Agent 一样看：Claude Code 工具设计实践 来自 Claude Code 开发者 @trq212，记录了 Anthropic 团队在开发 Claude Code 过程中关于 Agent 工具设计 的实践经验，有一个很核心的观点「开发者需要学会"像 Agent 一样看世界"」 第一层：工具设计的基本框架 如何为 Agent 设计动作空间？看一个数学题的比喻： · 纸笔：最低门槛，受限于手动计算 · 计算器：更强，需要知道如何操作 · 高级功能计算机：最强大，需要会编程 指向一个设计原则：工具应当与使用者的能力相匹配。给一个不会编程的人一台电脑，不如给他一个计算器。同理，给模型的工具必须是它能理解和有效调用的。 这里隐含了一个重要判断——工具不是越多越好，也不是越通用越好，要"shaped to its own abilities"。 第二层：AskUserQuestion 工具的三次迭代 尝试 1：在 ExitPlanTool 中附加问题参数 最省事的做法——复用现有工具。但失败了，原因是语义冲突：一个工具同时承担"输出计划"和"提出疑问"两个职责，模型会困惑。如果用户的回答推翻了计划怎么办？模型是否需要再调用一次？一个工具承载两个意图，会让模型无法形成清晰的调用决策。 尝试 2：修改输出格式（结构化 Markdown） 思路是让模型在普通文本输出中嵌入特定格式的问题，然后由前端解析。这是最"通用"的方案，不需要新增工具。但模型的输出不够稳定——会多加句子、遗漏选项、改变格式。自由文本输出的可靠性不足以支撑结构化交互。 尝试 3：独立的 AskUserQuestion Tool 最终方案是创建专用工具，在 plan mode 中尤其鼓励使用。调用时弹出模态框，阻塞 Agent 循环直到用户作答。 这个方案成功的关键在于三点： · 结构化输出——工具的参数 schema 强制模型给出选项，而非自由发挥 · 可组合性——可以在 Agent SDK 和 Skills 中引用 · 模型的自然倾向——"Claude seemed to like calling this tool"，模型对这个工具有良好的调用直觉 最后一点尤其值得注意。Thariq 明确说："Even the best designed tool doesn't work if Claude doesn't understand how to call it." 工具设计不仅是工程问题，还是与模型认知特性的匹配问题。 第三层：工具需要随模型能力进化 TodoWrite 到 Task Tool 的演变揭示了一条重要规律：曾经必要的工具，可能随着模型进步反而成为约束。 早期模型容易"忘记"待办事项，所以需要 TodoWrite 来追踪任务，并且每 5 轮插入系统提醒。但随着模型能力提升： · 模型不再需要被提醒 Todo 列表 · 系统提醒反而让模型过于僵化地遵循列表，而不是灵活调整 · Opus 4.5 对 subagent 的调度能力大幅提升，但 Todo 列表无法在多个 subagent 之间协调 于是 TodoWrite 被 Task Tool 取代。Task Tool 的核心转变是：从"帮模型记住事情"变为"帮 Agent 之间通信"——支持依赖关系、跨 subagent 同步、动态增删任务。 这里的教训很直接：定期回顾你对工具的假设。模型在变，你的工具也必须跟着变。 第四层：从 RAG 到自主搜索——上下文构建的范式转移 阶段 1：RAG 向量数据库 -> 被动接收上下文，需要索引和配置，环境兼容性差 阶段 2：Grep 工具 -> 模型主动搜索代码库，自己构建上下文 阶段 3：渐进式披露 -> 模型读取 Skill 文件，文件中引用其他文件，模型递归地展开搜索链条 这个演进的本质是：从"给模型喂上下文"到"让模型自己找上下文"。随着模型推理能力增强，它越来越擅长判断自己缺什么信息、去哪里找、找到后如何利用。 第五层：渐进式披露——不加工具也能扩展能力 Claude Code 目前约有 20 个工具，团队对新增工具的门槛很高，因为每多一个工具就多一个模型需要权衡的选项。 以"Claude Code 自身使用说明"为例： · 写入系统提示词：用户很少问这类问题，常驻上下文造成 context rot · 给模型文档链接让它自行加载：模型会加载大量结果，污染上下文 · 专用 Guide subagent：由 subagent 搜索文档并只返回精准答案 最终方案是 Guide subagent：不增加新工具，而是通过 subagent + 专用指令来扩展能力。这就是 Progressive Disclosure 的应用——在模型需要的时候，才逐层展开信息，而不是一次性塞入所有可能用到的知识。 这个思路对所有 Agent 开发者都有参考价值：能用信息架构解决的问题，就不要用新工具解决。 对 Agent 开发者的启示 以 Claude Code 为案例，方法论适用于所有 Agent 系统的构建： · 先理解模型的能力边界，再设计工具，而非反过来 · 一个工具只做一件事，语义清晰，边界分明 · 结构化胜过自由文本，尤其在需要可靠输出的场景 · 渐进披露优于一次性加载，信息架构本身就是一种工具 · 定期审视既有工具，模型在进化，工具也必须跟上 · 读模型的输出——这是理解模型"如何看世界"的唯一途径 正如 Thariq 所言，这是一门手艺（art），不是一套公式（science）。核心方法论只有一句话：学会像 Agent 一样看。 [图片: https://pbs.twimg.com/media/HCNayNbbEAAij9O?format=jpg&#x26;name=orig] Thariq: http://x.com/i/article/2027446899310313472

【9】群众使用龙虾的目的如此朴实（显然是不能实现的）。软件的增量市场不在给人提供服务，而在给Agent提供服务。 例如给Agent提供的免费搜索API里加广告也不是不能接...
群众使用龙虾的目的如此朴实（显然是不能实现的）。软件的增量市场不在给人提供服务，而在给Agent提供服务。 例如给Agent提供的免费搜索API里加广告也不是不能接受，免费有广告，付费高质量。 [图片: https://pbs.twimg.com/media/HCNXUHbbwAEsCCY?format=jpg&#x26;name=orig]

【10】新成为 尼日利亚 数字居民 😂 ChatGPT Plus 订阅仅需 ¥50/月
新成为 尼日利亚 数字居民 😂 ChatGPT Plus 订阅仅需 ¥50/月 [图片: https://pbs.twimg.com/media/HCNVmSVbEAIOlFN?format=jpg&#x26;name=orig]

【11】我最喜欢看的 YouTube 频道之一，这周被HubSpot收购了 频道主理人 @thepatwalls 要举办一场线上的分享，有兴趣的可以去看看 8年前，Starter Story在一家小小的星...
我最喜欢看的 YouTube 频道之一，这周被HubSpot收购了 频道主理人 @thepatwalls 要举办一场线上的分享，有兴趣的可以去看看 8年前，Starter Story在一家小小的星巴克开始。 这将是唯一一次帕特·沃尔斯现场深入讲解真正让它具有吸引力的因素，以及创业者在最初几年犯下的那些悄然限制他们潜力的错误。 如果你现在正在创业，这次会议可能会为你节省数年的时光。 https://www.youtube.com/watch?v=v-uhjlMg9L0

【12】最近在用网易的 LobsterAI，看更新日志他们这段时间在优化 Skills，内置了文档处理、前端设计、浏览器自动化、网络搜索、甚至有找电影资源这些模块，场景现成，...
最近在用网易的 LobsterAI，看更新日志他们这段时间在优化 Skills，内置了文档处理、前端设计、浏览器自动化、网络搜索、甚至有找电影资源这些模块，场景现成，不少场景一句话就能跑通。 这次试了个有意思的组合，最近对太平年这部剧很感兴趣，想把五代十国这段历史补一补，就直接丢了一个提示词：「把五代十国的脉络和关键知识点整理成一份 PDF，信息密度高一点，顺便帮我查一下太平年是否有云盘网络资源」。它调用了 web-search、pdf、films-search 三个 Skills，PDF 直接生成出来了，脉络清晰，排版也还不错；资源那边也真的搜出了下载地址，省掉了自己去各个网站翻的时间。 Skills 省掉的是拼工具链、写脚本、洗数据的时间。对工程师来说，就是把以前要搭工作流的事变成直接调用，去重、格式化、摘要也一并处理。组合起来能做的事情更多，比如盯着某个竞品每周的更新动态，用 web-search 抓完，直接用 playwright 提取关键变更，最后生成一份竞品周报，整条链路在一个地方跑完。感兴趣的可以从自己最高频的重复场景开始跑，推荐去玩玩 LobsterAI 最新的 Skills 功能。 https://lobsterai.youdao.com [视频: https://video.twimg.com/amplify_video/2027401115231567872/vid/avc1/2680x1856/Xa4FuBHMywDL4LGg.mp4?tag=21]

