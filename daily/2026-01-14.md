## AI洞察日报 2026/1/14

>  `AI 日报` 

### 今日摘要

【1】百川开源全球最强医疗大模型M3，「严肃问诊」定义AI医疗新能力
昨天，百川智能正式开源新一代医疗大模型 Baichuan-M3，其在全球最权威的医疗 AI 评测 HealthBench 中以 65.1 分的综合成绩位列全球第一；在专门考验复杂决策能力的 HealthBench Hard 上，也以 44.4 分的成绩夺冠。 这一成绩，不仅刷新了 HealthBench 的最高分，更首次在医疗领域实现了对 GPT-5.2 的全面超越。在 OpenAI 引以为傲的低幻觉领域，M3 也实现了超越，幻觉率 3.5 全球最低。 此外，M3 还首次具备了原生的 "端到端” 严肃问诊能力。它能像医生一样主动追问、逐层逼近，把关键病史和风险信号问出来，进而在完整的信息上进行深度医学推理。评测显示，其问诊能力显著高于真人医生的平均水平。 Hugging Face 地址：https://huggingface.co/baichuan-inc/Baichuan-M3-235B GitHub 地址：https://github.com/baichuan-inc/Baichuan-M3-235B 医疗沟通和推理能力超越 GPT-5.2，登顶世界第一 2025 年 5 月份，OpenAI 发布 HealthBench，由 262 位来自 60 个国家的医生共同构建，收录了 5000 组高度逼真的多轮医疗对话，构建了全球最权威、也最贴近真实临床场景的医疗评测集。这一事件，被视为 OpenAI 在医疗领域开始 "重兵投入”，吹响进军医疗的号角。 相当长一段时间里，无论是 HealthBench 总分还是 HealthBench-Hard 子集， GPT 系列模型从未被超越。2025 年 8 月，百川开源医疗增强大模型 M2 在 HealthBench 上力压 gpt-oss-120B、DeepSeek-R1 等同期所有开源模型，并在 HealthBench Hard 上取得 34.7 分的成绩，仅次于 GPT-5，成为全球唯二突破 32 分的模型。 [图片: https://image.jiqizhixin.com/uploads/editor/af4c0af2-6825-42b4-a119-489baef87e5c/1768353665316.png]2025 年，强化学习无疑是新一代 Scaling Law 的技术中轴。在 M2 发布后的五个月里，百川智能对强化学习系统进行了全面升级，将原本以患者模拟器和静态 Rubric 为主的半动态反馈，升级为随模型能力不断演进的全动态 Verifier System。随着监督信号持续变细、变难，模型得以不断突破能力上限，使 M3 在复杂医学问题上的表现实现跃迁，不仅在 HealthBench 总分上超越 OpenAI 最新模型 GPT-5.2，也在 HealthBench Hard 上登顶，成为当前全球医疗沟通和推理能力最强的医疗大模型。 重构幻觉抑制的训练范式，刷新医疗幻觉率底线 幻觉是这一代大模型技术范式的通病，更是 AI 进入严肃医疗的拦路虎。在大多数场景幻觉只是体验问题，而在严肃医疗场景可导致安全事件。 降低幻觉，一直是 OpenAI 最重视的研究方向之一。几乎每一代 GPT 模型的幻觉率均为行业最低。OpenAI 也是第一个单独评测医疗能力和提供医疗服务的通用模型公司。 国内 DeepSeek 等模型的普及，让越来越多人开始使用 AI 并尝试进行医疗健康咨询。但大多数模型公司并没有把 "降幻觉” 提升到与推理、代码等相同的高度。用这样的模型获取健康咨询和诊疗建议，对 AI 医疗的普及和医患信任建立带来很大困扰。 百川 M3 将医疗幻觉抑制前移至模型训练阶段，在强化学习过程中将医学事实一致性作为核心训练目标之一，将 "知之为知之，不知为不知” 直接作用于模型自身能力的形成过程。这一新的训练方法将医学事实可靠性内化为 M3 自身的基础能力，使其在不借助任何外部系统的情况下，依然能够基于自身医学知识进行稳定、可信的作答。 通过将事实一致性约束融入训练流程，M3 重构了幻觉抑制的训练范式，在不依赖工具或检索增强的纯模型设置下，医疗幻觉率 3.5，超越 GPT-5.2，达到全球最低水平。 [图片: https://image.jiqizhixin.com/uploads/editor/26310800-527a-4066-a8ff-63241be5aea9/1768353715138.png] 构建「严肃问诊」新能力，端到端问诊超越真人医生 除了强推理和低幻觉，端到端的问诊能力是本次 M3 最重要的一项突破。2025 年行业的技术共识是，用户提供更完整的上下文，模型才有更好的表现。可在医疗领域，患者很难完整表达自己的病症，需要模型像医生一样有能力把患者的混乱叙述转变成可做诊疗决策的信息。 HealthBench 代表了 OpenAI 对临床场景的认知高度，然而它本质上是一个切片式的评测，考核的更像是 "AI 会不会回答问题”，而不是带着诊疗目标，完整的患者信息收集。这也正说明了行业对问诊重要性和建模思路的理解不足。 应用实践中，通过 prompt "你是一位经验丰富的医生”，激活模型的 "角色扮演” 是更常见的做法。这种方式得到的是模型的表演行为，而非内生能力，激活的是模型应该提问的行为，而不是必须获取关键信息的思考。例如，临床医生面对患者的第一反应，永远是先排除危急重症，再考虑常规诊疗，这是刻在职业本能里的安全优先级。但常见的 "角色扮演” 的问诊方式，无法将 "红旗征识别与处置” 作为核心行动原则。这种不围绕关键风险点展开的信息收集，即便对话看似完整，也难以支撑安全、可靠的临床判断，从根本上偏离了医疗 "安全第一” 的原则。 针对这一行业困境，百川智能提出了 "严肃问诊范式” 与 "SCAN 原则”，通过 Safety Stratification（安全分层）、Clarity Matters（信息澄清）、Association &#x26; Inquiry（关联追问）与 Normative Protocol（规范化输出），将临床问诊中高度依赖经验的思维过程，第一次系统性地 "白盒化”。 围绕 SCAN 原则，百川智能借鉴医学教育里长期使用的 OSCE 方法，联合 150 多位一线医生，搭建了 SCAN-bench 评测体系，该体系以真实临床经验作为 "标准答案”，将诊疗过程拆解为病史采集、辅助检查、精准诊断三大阶段，通过动态、多轮的方式进行考核，完整模拟医生从接诊到确诊的全过程。相比于 HealthBench，SCAN-bench 是更加全流程端到端的动态评测新范式。 同时，百川智能还使用原生模型训练方法取代角色扮演 prompt，针对 GRPO 无法稳定进行长对话训练的问题，设计了新的 SPAR 算法，使模型能够在有限对话轮次中，把临床真正需要的关键问题问全、问准，把风险兜住，让输出经得起复核。 在实验过程中发现，问诊准确度每增加 2%，诊疗结果准确度就会增加 1%。评测结果显示，M3 在 SCAN 的四个维度均显著高于人类医生基线水平，并大幅领先于国内外顶尖模型，成功构建了从精准的临床问询、深度医学推理到安全可靠决策的闭环。 [图片: https://image.jiqizhixin.com/uploads/editor/f0ee5beb-e7ff-4129-87fe-9ea83c3ad5b4/1768353751451.png] 从 1 月初 OpenAI 发布医疗产品 ChatGPT Health，到今天 Anthropic 推出 Claude for Healthcare，AI 医疗正在全球范围内提档加速，竞争也正式进入深水区。在这场竞速中，作为国内唯一专注医疗的大模型企业，百川持续突破低幻觉率、端到端问诊和复杂临床推理等核心能力，已从 "跟随者” 跃迁为行业 "引领者” 与新范式的 "定义者”，正以硬核实力扛起中国 AI 医疗发展的旗帜。 百川智能的医疗应用 "百小应” 已同步接入 M3，面向医生与患者开放相关能力。医生可借助它推演问诊与诊疗思路，患者及家属也可通过该应用更系统地理解诊断、治疗、检查与预后背后的医学逻辑。 ]]>

【2】Salesforce 联手 Anthropic:全新 AI 助推器上线，让 Slack 成为你的企业大脑
办公协同巨头 Salesforce 近日宣布推出基于 Anthropic Claude 模型的全新 Slack 机器人，标志着其实战化 AI 布局的又一里程碑。这款深度集成的人工智能助手直接运行于 Slack 平台，彻底打破了传统应用间的信息壁垒。它不仅能够实时搜索 Slack 内部的对话与文件，更打通了 Salesforce、Google Drive、Box 以及 Atlassian Confluence 等多平台数据，利用多维度的上下文信息协助用户准备会议、创建内容并精准回答复杂问题。 [图片: QQ20260114-092008.png https://upload.chinaz.com/2026/0114/6390397932255682978502417.png] Slack 联合创始人兼首席技术官 Parker Harris 指出，虽然目前优先采用 Claude 模型，但公司仍在积极测试其他技术方案以保持灵活性。值得注意的是，这款新助手在大幅提升工作效率的同时，严格遵循企业现有的访问权限协议，确保数据安全合规。 目前，该功能已向 Business+ 和 Enterprise+ 客户开放，并计划于2月份全面推广。未来，这款机器人将进一步整合 Agentforce 及其他 AI 代理，从单一的任务助手演变为能够协同复杂工作流的智能终端。

【3】全球首款医疗大模型 Baichuan-M3 亮相：超越 GPT-5.2，实力不容小觑！
近日，国产医疗大模型 Baichuan-M3正式发布，成为全球 最强 的医疗 AI 系统。这款模型由百川智能推出，经过深度优化，专注于医疗场景的应用，融合了大量医学文献、临床指南、真实病历以及药品知识库，展现了惊人的智能医疗能力。 Baichuan-M3的参数高达2350亿，核心优势在于其超低的幻觉率。这意味着在进行医疗问诊和提供用药建议时，Baichuan-M3不仅具备高度的准确性，还能有效避免错误信息的产生。根据评测结果，该模型在问诊能力和医疗准确性方面均超越了 OpenAI 的 GPT-5.2，并在各项评估中都优于人类医生。 [图片: image.png https://upload.chinaz.com/2026/0114/6390397923980337687263743.png] 百川智能的创始人王小川表示，Baichuan-M3的发布将推动医疗 AI 生态的共建。该模型的开源策略也将鼓励更多开发者参与到医疗 AI 的创新中，力求在基层医疗、辅助诊断以及健康管理等场景中实现落地应用。 [图片: image.png https://upload.chinaz.com/2026/0114/6390397925010303022153000.png] 目前，Baichuan-M3已在百小应平台上开放供用户体验，用户可以通过这个平台获得用药指导及其他医疗相关的帮助。这一创新不仅为患者提供了更为便捷的医疗咨询渠道，也为医生的工作提供了有力的支持。 随着医疗 AI 技术的发展，像 Baichuan-M3这样的模型将越来越多地被应用于医疗领域，未来有望进一步提升医疗服务的质量和效率，造福更多人群。

【4】国产算力+自主创新架构！智谱联合华为开源GLM-Image，首个多模态SOTA模型全链路跑通昇腾芯片
近日，智谱AI与华为联合宣布开源新一代图像生成大模型 GLM-Image，该模型不仅在性能上达到当前国际领先水平（SOTA），更创下一项关键纪录：全球首个从数据处理、训练到推理全流程均基于国产AI芯片完成的多模态大模型。 据悉，GLM-Image全程依托华为昇腾Atlas 800T A2 服务器与昇思MindSpore AI框架构建，彻底摆脱对国外GPU及深度学习框架的依赖，验证了国产软硬件栈支撑 尖端 AI研发的可行性与成熟度。 技术层面，GLM-Image采用智谱自主研发的 "自回归+扩散解码器”混合架构，巧妙融合语言建模的逻辑连贯性与扩散模型的高保真生成能力。这一设计使其不仅能根据文本精准生成高质量图像，还能实现图文语义的深度对齐与联合推理，为"认知型生成”（Cognitive Generation）这一新兴范式提供核心引擎。该技术路线正被应用于以Nano Banana Pro为代表的下一代AI创作平台，推动AIGC从"像素堆砌”迈向"语义驱动”。 此次合作标志着国产AI生态正从"可用”走向"好用”。过去，高性能多模态模型几乎全部依赖英伟达GPU与PyTorch/TensorFlow生态；如今，GLM-Image的成功训练证明，基于昇腾+MindSpore的全栈国产方案已具备支撑前沿科研与产业落地的能力。 在中美科技竞争加剧、算力自主可控成为国家战略的背景下，GLM-Image的发布不仅是一次技术成果展示，更是中国AI产业链协同创新的关键一步。随着更多开发者基于该模型进行微调与应用开发，一个真正自主、开放、高性能的中文多模态生态有望加速成型。

【5】Anthropic 重组高管团队，助力内部创新孵化器发展
Instagram 联合创始人 Mike Krieger 在加入 AI 初创公司 Anthropic 两年后，正在进行职位调整，转而共同领导公司的内部孵化器 "Labs” 团队。Krieger 之前担任公司首席产品官，他的新角色将专注于推动 "实验性产品” 的开发。 "Labs” 团队于2024年中期成立，最初仅有两名成员。如今，Anthropic 决定扩展该团队规模，计划在未来六个月内将团队人数翻倍。Krieger 将成为技术团队的一员，向公司总裁 Daniela Amodei 汇报，并与现任产品工程负责人 Ben Mann 共同领导 "Labs” 团队。与此同时，现任 "产品负责人” Ami Vora 将接替 Krieger 的职责，并与首席技术官 Rahul Patil 密切合作，推动公司的产品扩展。 Krieger 在接受采访时表示:"我们正处于人工智能的关键时刻，模型能力迅速提升，塑造它们应用的机会窗口已到。这就是我为何决定回归开发者的角色，加入我们的‘Labs’团队。我希望在前沿领域亲自参与，构建能够应对全球最棘手问题的产品。我很高兴将接力棒交给 Ami，她将领导团队推动 Claude 的扩展。” 此次高管调整恰逢 AI 初创企业与科技巨头之间竞争加剧之际，Anthropic 正试图通过内部创新推动公司向前发展。 划重点: - 🚀 Mike Krieger 将从首席产品官转型，领导 Anthropic 的 "Labs” 团队，专注于实验性产品开发。 - 📈 Anthropic 计划在未来六个月内将 "Labs” 团队人数翻倍，以加速创新。 - 🌍 Krieger 强调人工智能发展的关键时刻，表达了对推动 AI 解决全球问题的热情。

【6】🔧 40 行修复：消除 JVM 线程计时引起的 400x 性能差距
原标题： 《A 40-Line Fix Eliminated a 400x Performance Gap》 评分: 41 | 作者: bluestreak 💭 40 行就省下 400x，内核在度假吗？ 🎯 讨论背景 一篇技术贴报告通过约 40 行代码修复，消除了一个由 JVM 获取线程 CPU 时间导致的巨大性能差距（标题称约 400x）。讨论围绕用户态与内核态计时实现差异展开：clock_gettime() 在某些时钟源上可通过 vDSO（Linux 的用户态快速路径）避免系统调用，但对 per-thread 计时（CLOCK_THREAD_CPUTIME_ID）通常回退到内核。有人提出使用 Linux perf（如 PERF_COUNT_SW_TASK_CLOCK 与 perf_event_mmap_page）结合 rdtsc 与 seqlock 在用户态推导线程时间作为更激进的优化方案，但该路径文档不足且实现复杂。评论还指出基准在非隔离环境下容易产生噪声，强调对时钟精度与测试环境做更严格控制以验证纳秒级改动。 📌 讨论焦点 根因：JVM 查询线程 CPU 时间代价高 作者在追踪性能问题时发现，JVM 对"某线程的 CPU 时间是多少”这一查询的实现代价远高于预期，成为性能瓶颈的主要来源。按线程计时通常需要内核访问任务结构（task struct），因此该查询常常回退到内核路径并触发系统调用，带来显著开销。文章标题指出通过约 40 行代码的修复消除了约 400x 的性能差距，评论中也确认这是一个被低估的高开销问题。围绕这一发现，讨论扩展到内核/用户态计时实现与优化策略的选择。 [来源1] [来源2] [来源3] vDSO 与 CLOCK_THREAD_CPUTIME_ID 的局限 Linux 的 vDSO（virtual dynamic shared object）允许部分时钟（如 CLOCK_MONOTONIC）在用户态快速返回，从而避免上下文切换和系统调用。评论指出这种加速并不普遍适用于所有 clock id，尤其是 CLOCK_THREAD_CPUTIME_ID 这类需要每线程计数的时钟，vDSO shim 常常回退到内核实现。在 flamegraph 中可以看到 vDSO 帧下仍存在系统调用，说明实现缺少针对该 clock id 的快速路径。因此即便调用了 clock_gettime()，对线程级 CPU 计时的请求仍可能落入昂贵的内核路径。 [来源1] [来源2] [来源3] [来源4] 替代优化：使用 perf 软件事件和共享页绕过 syscall 有评论建议用 Linux perf 的软件事件 PERF_COUNT_SW_TASK_CLOCK 来直接获得线程 CPU 时间，通过 perf_event_mmap_page 暴露的共享页在用户态读取可以避免每次发起系统调用。配合一次 rdtsc（读取 CPU 时间戳计数器）并在 seqlock（顺序锁）内计算自上次上下文切换以来的增量，据称能把开销再减少一个数量级、达到约 7ns 的量级。该方法被描述为能带来约 10x 的额外提升，但同时被警告文档不足、实现复杂且缺乏开源范例，存在可移植性和同步问题需要处理。因此评论把它当作更激进但有吸引力的优化方向，并提醒谨慎实现。 [来源1] 基准测量的准确性与噪声问题 一些评论质疑在纳秒尺度讨论改进的可靠性，指出在此级别需要对时钟的稳定性和准确度有深入验证，否则测量误差可能掩盖真实效果。也有人强调在非隔离的开发工作站上跑基准会有大量中断和其他任务干扰，导致分布波动甚大甚至跨数量级，文章中的分布和离群点提示需要在更受控环境下复现。另一方面，评论也提出在将纳秒差异放到毫秒或微秒级别对比时，普通晶振通常足够，但对极小百分比差异仍需大量重复与严格控制变量。总体建议是改进测量方法、隔离测试环境并报告分布与统计指标而非单一均值。 [来源1] [来源2] [来源3] [来源4] [来源5] 社区反应：写作风格与 TLDR 受欢迎 多名评论者对这篇技术写作表示肯定，尤其赞赏作者或评论中提供的简短 TLDR 一行总结，认为在 Hacker News 这样的环境里能快速抓住要点非常有价值。有人提到短小要点适合在加载模型或等待短时间窗口时阅读，能显著提升信息吸收效率和传播率。回复显示这种"先给一个一行结论、再提供细节”的格式受欢迎，社区希望看到清晰、可复现的修复说明和实用建议。总体反响既有技术深挖也有人情化的阅读体验反馈。 [来源1] [来源2] [来源3] [来源4] 📚 术语解释 vDSO: vDSO（virtual dynamic shared object）：Linux 在用户空间提供的一块共享页/库，用于实现部分系统调用的用户态快速路径（例如某些 clock_gettime），以避免内核上下文切换，但不一定对每种 clock id 提供快速路径。 PERF_COUNT_SW_TASK_CLOCK: PERF_COUNT_SW_TASK_CLOCK：Linux perf 的一个 software event，用于计数线程级的 CPU 时间消耗，可与 perf_event_mmap_page 配合在用户态读取以减少系统调用开销。 perf_event_mmap_page: perf_event_mmap_page：Linux perf 子系统通过 mmap 暴露的一块共享内存页，用户态程序可在不发 syscall 的情况下读取性能计数器和时间戳，但需正确的同步与文档约束。 rdtsc: rdtsc：x86 指令，读取处理器的时间戳计数器（TSC），能提供高分辨率时间戳，但需处理核心迁移、TSC 同步与序列性问题。 seqlock: seqlock（顺序锁）：一种读多写少的同步机制，读者通过检查序号来保证读取一致性，常用于在不阻塞读方的情况下与写方同步共享页（如 perf_event_mmap_page）。 CLOCK_THREAD_CPUTIME_ID: CLOCK_THREAD_CPUTIME_ID：POSIX 的时钟 id，用于查询单个线程的 CPU 时间。该查询通常需要访问内核的任务结构，因此 vDSO 可能不会为其提供快速路径，可能会触发系统调用。

【7】superpowers
Claude Code 超级能力：核心技能库

【8】icloud_photos_downloader
一个从 iCloud 下载照片的命令行工具

【9】frigate
支持 IP 摄像头实时本地物体检测的网络视频录像机

【10】the-algorithm
X 推荐算法源代码

【11】home-assistant.io
📘 Home Assistant 用户文档

【12】buzz
Buzz 可在您的个人电脑上离线转录和翻译音频。由 OpenAI 的 Whisper 驱动。

【13】Browser Use 推出「BU」，要取代 Manus 🧐 @browser_use 团队 Manus 不过是 Browser Use 的套壳，他们可以做得更好，效果可以先看官方视频。 现在 BU 还是 Wai...
Browser Use 推出「BU」，要取代 Manus 🧐 @browser_use 团队 Manus 不过是 Browser Use 的套壳，他们可以做得更好，效果可以先看官方视频。 现在 BU 还是 Waitlist 阶段，加入和排序方式也很有趣，大家还记得 Chrome 断网后的游戏吗，是的，就是这个跑酷小游戏，得分越高，等待排名越靠前。我这个手残党是没希望了。。 https://bu.app/play [图片: https://pbs.twimg.com/media/G-lk0QlXUAA2oyy?format=jpg&#x26;name=orig] Browser Use: Today we’re launching BU [beta]. Meta paid $2B for Manus - the browser use wrapper. We replace them. Here's Manus vs BU: The web agent of the future. [视频: https://video.twimg.com/amplify_video/2011211864945160192/vid/avc1/1920x1080/dHj96_Xio2oudQJm.mp4?tag=21]

【14】如何利用 Claude Code 和 Claude Opus 4.5 短短 5 天内构建出 Learning Machines -- 来自 @AgnoAgi 创始人 @ashpreetbedi 的实战分享 核心方法论："规格说明优...
如何利用 Claude Code 和 Claude Opus 4.5 短短 5 天内构建出 Learning Machines -- 来自 @AgnoAgi 创始人 @ashpreetbedi 的实战分享 核心方法论："规格说明优先”开发 Bedi 认为，使用 AI 编程工具最常见的失败原因是上下文混乱。他通过建立一套标准化的文档体系，将"意图”与"实现”彻底分离。 1. 外部存储与软链接 · 做法：创建一个独立的 specs/ 仓库，通过 ln -s 软链接到项目目录，并将其加入 .gitignore。 · 目的：让 AI 能读取规格说明，但不会将这些频繁变动的辅助文档混入主项目的 Git 提交历史。 · 文档结构： · design. md：单一事实来源，开发前必须对齐。 · implementation. md：动态追踪进度，解决 AI 因上下文长度限制需要重启会话时的断点续传问题。 · decisions. md：记录决策理由，防止 AI 或人类在后续迭代中推翻先前的架构逻辑。 · prompts. md：沉淀可复用的高质量提示词。 2. 分层指令系统 利用了 Claude Code 自动读取 CLAUDE. md 的特性，构建了双层治理结构： · 根目录级别：定义全局规范（代码位置、禁止事项、通用架构模式）。 · 功能模块级别：定义特定功能的上下文（参考实现、特定协议、检查清单）。 · 价值：这类似于为 AI 提供了"短期记忆”与"长期记忆”的结合，确保 AI 在导航大规模代码库时不迷失方向。 工作流转换：从"写作者”到"评审员” Bedi 的身份转变代表了 AI 时代程序员的新形态：不再是代码的生产者，而是系统设计的决策者和代码质量的守门人。 关键环节流程： · 模糊输入：通过语音转文字（Whisper）快速录入原始想法。 · AI 建模：Claude 阅读代码库和 Spec，自动生成详细设计文档。 · 人类评审（核心环节）：这是 Bedi 投入精力最多的地方，确保设计无误。 · 原子化实现：要求 AI 每次只完成一个小功能块。硬性约束：每个 PR 必须在 10 分钟内评审完（&#x3C;500 行，&#x3C;7 个文件）。 · Cookbook 验证："不跑通就不算完”。要求 AI 编写可运行的示例并运行，将结果记录在 TESTING. md 中。 专家视角的工具见解 · 模型选择：他高度评价 Opus 4.5，认为其逻辑深度足以处理高性能、高性能关键型应用（如 Agno 的多智能体运行时）。 · 计划模式：强调在实施前必须进入"计划模式”。直接写代码往往导致低质量输出，而 5 分钟的架构规划能节省数小时的调试时间。 · 上下文管理：他观察到当对话过长（约 30% 上下文占位后）模型性能会下降，因此主张"一个功能一个对话”，通过外部 Spec 文档保持状态。 [图片: https://pbs.twimg.com/media/G-ljQp3awAAbGE8?format=jpg&#x26;name=orig] Ashpreet Bedi: http://x.com/i/article/2011128658598248449

【15】[论文解读] BabyVision: 让 AI 能够像人类婴儿一样，在不具备成熟语言能力的情况下，通过纯视觉观察来理解物理世界和抽象逻辑，突破当前多模态模型对语言高度依...
[论文解读] BabyVision: 让 AI 能够像人类婴儿一样，在不具备成熟语言能力的情况下，通过纯视觉观察来理解物理世界和抽象逻辑，突破当前多模态模型对语言高度依赖，转向"超越语言的视觉推理”的前沿研究 @UniPat_AI 核心理念：超越语言的视觉推理 目前的主流多模态模型（如 GPT-4V, Gemini）通常将视觉信息转化为语言描述或通过语言引导的逻辑来解决问题 。BabyVision 认为这种"语言依赖”限制了 AI 处理那些难以用言语表达、但符合直觉和物理常识的视觉任务的能力 。 · 模拟婴儿认知：婴儿在学会说话前就能通过观察物体运动、形状变化和空间关系进行推理 。BabyVision 试图在 AI 中重现这种能力 。 · 解决"语言瓶颈”：避免在复杂视觉推理（如几何旋转、拓扑关系、隐藏物理过程）中因语言转换而产生的信息损失或幻觉 。 BabyVision 基准测试 为了衡量这种纯视觉推理能力，该项目提出了一个包含多样化任务的评估套件： · 任务维度： · 物理常识：考察模型对物体永存性、因果关系和重力等物理法则的理解 。 · 抽象逻辑：包括非语言的模式识别（类似于瑞文推理测验）和视觉类比 。 · 空间智能：考察三维旋转、透视变化和遮挡关系处理 。 · 数据特点：数据设计尽量去语言化，题目通常以图像序列或视觉问题呈现，要求模型仅凭视觉信息给出判断 。 实验结果与发现 · 现有多模态模型的局限性：即使是顶级模型，在面对完全排除语言提示、纯依赖视觉逻辑的任务时，表现往往显著下降 。 · 视觉直觉的缺失：目前 AI 更多是在"阅读”图像，而非"感知”物理世界。BabyVision 通过针对性训练，在不牺牲通用语言能力的前提下，提升了模型的视觉常识推理水平 。 行业意义 BabyVision 为多模态学习指明了一个新方向： · 具身智能：对于机器人而言，在物理环境中的快速反应往往依赖于视觉直觉而非冗长的语言推理，BabyVision 的研究成果对此至关重要。 · 模型评估新标准：它挑战了"语言能力强即多模态能力强”的现有偏见，为评估 AI 的"视觉大脑”提供了更纯粹的尺度。 论文：https://huggingface.co/papers/2601.06521 开源：https://github.com/UniPat-AI/BabyVision [图片: https://pbs.twimg.com/media/G-lhDYRbEAAaC-d?format=jpg&#x26;name=orig]

【16】[开源推荐] re-ink: @LandingAI Financial AI Hackathon Championship 入围决赛的项目，通过 AI 驱动的文档提取技术，自动化再保险合同的管理流程 re-ink 面对的...
[开源推荐] re-ink: @LandingAI Financial AI Hackathon Championship 入围决赛的项目，通过 AI 驱动的文档提取技术，自动化再保险合同的管理流程 re-ink 面对的问题 再保险合同通常长达 50 页以上，涉及条款、各方当事人、金融细节等复杂内容。传统流程要求人工阅读、提取和录入数据，这导致效率低下和人为错误。 re-ink 的解决方案 · 上传与提取：用户上传 PDF 或 Word 格式的合同，应用使用 AI 驱动的文档提取（ADE）自动解析结构，提取关键信息，包括：合同日期和条款、覆盖限额和保费、各方当事人（转让人、再保险人、中介）、金融细节。 · 审核与审批：提取数据显示在审核界面，用户可验证、编辑并批准。 · 存储与管理：批准后，数据自动流入 PostgreSQL 数据库，使合同可搜索和管理。 · 关键技术：ADE 采用视觉优先架构，将合同视为视觉结构而非纯文本，保留条款间的空间关系，提高复杂布局的解析准确性。 开源地址 https://github.com/vineetsarpal/re-ink [图片: https://pbs.twimg.com/media/G-letnMbAAA2pTs?format=jpg&#x26;name=orig] LandingAI: What if a 50-page reinsurance contract didn't require manual data entry? Right now, someone has to read through all the terms, identify every party, extract financial details, and enter everything manually. Every. Single. Contract. This is the bottleneck reinsurance teams face [图片: https://pbs.twimg.com/media/G-jtervakAACFzg?format=png&#x26;name=orig]

【17】这相当于是武功秘籍给到手
这相当于是武功秘籍给到手 Guillermo Rauch: We're encapsulating all our knowledge of @reactjs &#x26; @nextjs frontend optimization into a set of reusable skills for agents. This is a 10+ years of experience from the likes of @shuding, distilled for the benefit of every Ralph [图片: https://pbs.twimg.com/media/G-kk8QGbQAAt2vb?format=jpg&#x26;name=orig]

【18】[D] TMLR timeline question: how long after rebuttal is it normal to wait for a decision?
Hi everyone, I have a quick question about typical timelines for TMLR. I submitted a paper to TMLR, received reviews, and then submitted the rebuttal. It’s now been about 3 weeks since the rebuttal , and there hasn’t been any update yet. I understand TMLR is a journal with rolling submissions and no hard deadlines, so delays are expected. I’ve seen some mentions that the discussion/rebuttal phase is designed to last ~2–4 weeks , and that Action Editors may wait during this period for possible reviewer responses or official recommendations before making a decision. For those who’ve submitted to TMLR before: Is 3–4 weeks after rebuttal still considered normal? How long did it take for you to receive a decision after rebuttal? Just trying to calibrate expectations — not complaining. Thanks in advance! submitted by /u/SynagogueLog [link] [comments]

